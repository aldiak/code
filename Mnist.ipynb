{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def test_model():\n",
    "    \n",
    "\n",
    "    net = Net()\n",
    "    net.add_layer(FCLayer(28*28, 60, activation=ReLU))\n",
    "    # net.add_layer(FCLayer(28*28, 20))\n",
    "    # net.add_layer(FCLayer(20, 10))\n",
    "    net.add_layer(FCLayer(60, 10, activation=SoftMax))\n",
    "    # net.add_layer(FCLayer(20, 10))\n",
    "    net.compile()\n",
    "\n",
    "    net.train(X_train, y_train, X_test, y_test, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation function\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    x = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    s = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    return s\n",
    "\n",
    "class Sigmoid:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.last_forward = None\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        self.last_forward = sigmoid(in_data)\n",
    "        return self.last_forward\n",
    "\n",
    "    def derivative(self, in_data=None):\n",
    "        self.last_forward = self.forward(in_data) if in_data else self.last_forward\n",
    "        return self.last_forward * (1 - self.last_forward)\n",
    "\n",
    "class SoftMax:\n",
    "    def __init__(self):\n",
    "        self.last_forward = None\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        self.last_forward = softmax(in_data)\n",
    "        return self.last_forward\n",
    "\n",
    "    def derivative(self, in_data=None):\n",
    "        last_forward = in_data if in_data else self.last_forward\n",
    "        return np.ones(last_forward.shape)\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.last_forward = None\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        self.last_forward = in_data\n",
    "        return np.maximum(0.0, in_data)\n",
    "\n",
    "    def derivative(self, in_data=None):\n",
    "        res = np.zeros(self.last_forward.shape, dtype='float32')\n",
    "        res[self.last_forward > 0] = 1.\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost function\n",
    "\n",
    "class CrossEntropyCost:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(y, a):\n",
    "        # return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n",
    "        return np.mean(-np.sum(y * np.log(a), axis=1))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(y, a):\n",
    "        return a - y\n",
    "\n",
    "\n",
    "class QuadraticCost:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(a, y):\n",
    "        return 0.5 * np.linalg.norm(a - y) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(y, a):\n",
    "        return a - y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD optimizer\n",
    "\n",
    "class Optimizer(object):\n",
    "    def __init__(self, lr=0.001, clip=-1, decay=0., lr_min=0., lr_max=np.inf):\n",
    "        self.lr = lr\n",
    "        self.clip = clip\n",
    "        self.decay = decay\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "\n",
    "        self.iterations = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.iterations += 1\n",
    "\n",
    "        self.lr *= (1. / 1 + self.decay * self.iterations)\n",
    "        self.lr = np.clip(self.lr, self.lr_min, self.lr_max)\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        Optimizer.__init__(self)\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for p, g in zip(params, grads):\n",
    "            # p -= 0.3 * g\n",
    "            p -= self.lr * npdl_clip(g, self.clip)\n",
    "\n",
    "        super(SGD, self).update(params, grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layers\n",
    "\n",
    "class FCLayer:\n",
    "    def __init__(self, n_in, n_out, activation=Sigmoid):\n",
    "\n",
    "        # self.b = np.random.randn(1, n_out)\n",
    "        self.b = np.zeros((1, n_out), dtype='float32')\n",
    "        # self.b = Zero().call((n_out, ))\n",
    "        # self.w = np.random.randn(n_in, n_out)\n",
    "        self.w = np.random.randn(n_in, n_out) / np.sqrt(n_in)\n",
    "\n",
    "        self.ac_fn = activation()\n",
    "\n",
    "        self.d_w, self.d_b = None, None\n",
    "        self.last_input = None\n",
    "\n",
    "        self.b_first_layer = False\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        self.last_input = in_data\n",
    "\n",
    "        z = np.dot(in_data, self.w) + self.b\n",
    "        a = self.ac_fn.forward(z)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward(self, pre_grad):\n",
    "        act_grad = pre_grad * self.ac_fn.derivative()\n",
    "        self.d_w = np.dot(self.last_input.T, act_grad)\n",
    "        self.d_b = np.mean(act_grad, axis=0)\n",
    "\n",
    "        if not self.b_first_layer:\n",
    "            # return delta * w\n",
    "            return np.dot(act_grad, self.w.T)\n",
    "\n",
    "    # The below two function mainly used for update w, b\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.w, self.b\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        return self.d_w, self.d_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class neural network\n",
    "\n",
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, cost=CrossEntropyCost, optimizer=SGD):\n",
    "        self.cost = cost()\n",
    "        self.optimizer = optimizer()\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, y_test,\n",
    "              epochs=100, lr=0.5, batch_size=100):\n",
    "\n",
    "        method_name = self.optimizer.__class__.__name__\n",
    "        print (\"using %s method to train\" % method_name)\n",
    "\n",
    "        n = len(X_train)\n",
    "\n",
    "        lst_iter, lst_loss, lst_acc = [], [], []\n",
    "\n",
    "        for ep in xrange(epochs):\n",
    "            np.random.seed(ep)\n",
    "            arr_idx = np.arange(n)\n",
    "            np.random.shuffle(arr_idx)\n",
    "\n",
    "            for k in xrange(0, n, batch_size):\n",
    "\n",
    "                # forward propagation\n",
    "                y_pred = self.forward(X_train[k:k+batch_size])\n",
    "\n",
    "                # backward propagation\n",
    "                next_grad = self.cost.backward(y_train[k:k+batch_size], y_pred)\n",
    "                for layer in self.layers[::-1]:\n",
    "                    next_grad = layer.backward(next_grad)\n",
    "\n",
    "                # get parameter and gradients\n",
    "                params = []\n",
    "                grads = []\n",
    "                for layer in self.layers:\n",
    "                    params += layer.params\n",
    "                    grads += layer.grads\n",
    "\n",
    "                # update parameter\n",
    "                self.optimizer.update(params, grads)\n",
    "\n",
    "            # print info\n",
    "            print (\"============== epoch %s complete =============\" % ep)\n",
    "            cost = self.get_cost(X_train, y_train)\n",
    "            print (\"training cost is %s\" % cost)\n",
    "\n",
    "            right_num = self.get_accuracy(X_test, y_test)\n",
    "            print (\"accuracy on test data %s / %s\" % (right_num, len(y_test)))\n",
    "\n",
    "            lst_iter.append(ep)\n",
    "            lst_acc.append(1.0 * right_num / len(y_test))\n",
    "            lst_loss.append(cost)\n",
    "\n",
    "        draw_result(lst_iter, lst_loss, lst_acc, method_name)\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        x_in = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_in = layer.forward(x_in)\n",
    "\n",
    "        y_pred = x_in\n",
    "        return y_pred\n",
    "\n",
    "    def get_accuracy(self, X_batch, y_batch):\n",
    "        rets = [(np.argmax(self.forward(x)), np.argmax(y))\n",
    "                for (x, y) in zip(X_batch, y_batch)]\n",
    "\n",
    "        return sum(a == y for (a, y) in rets)\n",
    "\n",
    "    def get_cost(self, X_train, y_train):\n",
    "        a = self.forward(X_train)\n",
    "        return self.cost.forward(y_train, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation\n",
    "\n",
    "def draw_result(lst_iter, lst_loss, lst_acc, title):\n",
    "    plt.plot(lst_iter, lst_loss, '-b', label='loss')\n",
    "    plt.plot(lst_iter, lst_acc, '-r', label='accuracy')\n",
    "\n",
    "    plt.xlabel(\"n iteration\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(title)\n",
    "    plt.savefig(title+\".png\")  # should before show method\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#Corresponding test codeï¼š\n",
    "\n",
    "def test_draw():\n",
    "    lst_iter = range(100)\n",
    "    lst_loss = [0.01 * i - 0.01 * i ** 2 for i in xrange(100)]\n",
    "    # lst_loss = np.random.randn(1, 100).reshape((100, ))\n",
    "    lst_acc = [0.01 * i + 0.01 * i ** 2 for i in xrange(100)]\n",
    "    # lst_acc = np.random.randn(1, 100).reshape((100, ))\n",
    "    draw_result(lst_iter, lst_loss, lst_acc, \"sgd_method\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
