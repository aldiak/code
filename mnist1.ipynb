{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "Y_tr_resh = y_train.reshape(60000, 1)\n",
    "Y_te_resh = y_test.reshape(10000, 1)\n",
    "Y_tr_T = to_categorical(Y_tr_resh, num_classes=10)\n",
    "Y_te_T = to_categorical(Y_te_resh, num_classes=10)\n",
    "y_train = Y_tr_T.T\n",
    "y_test = Y_te_T.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = x_train.reshape(x_train.shape[0], -1).T\n",
    "X_test_flatten = x_test.reshape(x_test.shape[0], -1).T\n",
    "x_train = X_train_flatten / 255.\n",
    "x_test = X_test_flatten / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nn_architecture = [\n",
    "\n",
    "{\"layer_size\": 4,\"activation\": \"none\"}, # input layer\n",
    "\n",
    "{\"layer_size\": 5,\"activation\": \"relu\"},\n",
    "\n",
    "{\"layer_size\": 4,\"activation\": \"relu\"},\n",
    "\n",
    "{\"layer_size\": 3,\"activation\": \"relu\"},\n",
    "\n",
    "{\"layer_size\": 1,\"activation\": \"sigmoid\"}\n",
    "\n",
    "]\n",
    "\n",
    "def initialize_parameters(nn_architecture, seed = 3):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# python dictionary containingour parameters \"W1\", \"b1\", ..., \"WL\",\"bL\"\n",
    "\n",
    "    parameters = {}\n",
    "\n",
    "    number_of_layers = len(nn_architecture)\n",
    "\n",
    "    for l in range(1,number_of_layers):\n",
    "\n",
    "        parameters['W' + str(l)] =np.random.randn(\n",
    "\n",
    "        nn_architecture[l][\"layer_size\"],\n",
    "\n",
    "        nn_architecture[l-1][\"layer_size\"]) * 0.01\n",
    "\n",
    "        parameters['b' + str(l)] =np.zeros((nn_architecture[l][\"layer_size\"], 1))\n",
    "\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation functions\n",
    "\n",
    "def sigmoid(Z):\n",
    "\n",
    "    S = 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    return S\n",
    "\n",
    "def relu(Z):\n",
    "\n",
    "    R = np.maximum(0, Z)\n",
    "\n",
    "    return R\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "\n",
    "    S = sigmoid(Z)\n",
    "\n",
    "    dS = S * (1 - S)\n",
    "\n",
    "    return dA * dS\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "\n",
    "    dZ = np.array(dA, copy = True)\n",
    "\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code segment 2, you can see the activation function and its derived vectorized programming implementation. This code will be used for further calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, nn_architecture):\n",
    "\n",
    "    forward_cache = {}\n",
    "\n",
    "    A = X\n",
    "\n",
    "    number_of_layers =len(nn_architecture)\n",
    "\n",
    "    for l in range(1,number_of_layers):\n",
    "\n",
    "        A_prev = A\n",
    "\n",
    "        W = parameters['W' + str(l)]\n",
    "\n",
    "        b = parameters['b' + str(l)]\n",
    "\n",
    "        activation =nn_architecture[l][\"activation\"]\n",
    "\n",
    "        Z, A =linear_activation_forward(A_prev, W, b, activation)\n",
    "\n",
    "        forward_cache['Z' + str(l)] =Z\n",
    "\n",
    "        forward_cache['A' + str(l)] =A\n",
    "\n",
    "        AL = A\n",
    "\n",
    "        return AL, forward_cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    if activation ==\"sigmoid\":\n",
    "\n",
    "        Z = linear_forward(A_prev, W,b)\n",
    "\n",
    "        A = sigmoid(Z)\n",
    "\n",
    "    elif activation ==\"relu\":\n",
    "\n",
    "        Z = linear_forward(A_prev, W,b)\n",
    "\n",
    "        A = relu(Z)\n",
    "\n",
    "        return Z, A\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Code segment 3 forward propagation model\n",
    "\n",
    "Use \"cache\" (python dictionary contains a and z values ​​calculated for a particular layer) to pass variables during forward propagation to the corresponding backpropagation. It contains useful values ​​for calculating the derivative of the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost computation\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "# Compute loss from AL and y\n",
    "\n",
    "    logprobs =np.multiply(np.log(AL),Y) + np.multiply(1 - Y, np.log(1 - AL))\n",
    "\n",
    "# cross-entropy cost\n",
    "\n",
    "    cost = - np.sum(logprobs) / m\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, parameters, forward_cache, nn_architecture):\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    number_of_layers =len(nn_architecture)\n",
    "\n",
    "    m = AL.shape[1]\n",
    "\n",
    "    Y = Y.reshape(AL.shape) # afterthis line, Y is the same shape as AL\n",
    "\n",
    "# Initializing thebackpropagation\n",
    "\n",
    "    dAL = - (np.divide(Y, AL) -np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    dA_prev = dAL\n",
    "    \n",
    "    for l in reversed(range(1,number_of_layers)):\n",
    "\n",
    "        dA_curr = dA_prev\n",
    "\n",
    "        activation =nn_architecture[l][\"activation\"]\n",
    "\n",
    "        W_curr = parameters['W' +str(l)]\n",
    "\n",
    "        Z_curr = forward_cache['Z' +str(l)]\n",
    "\n",
    "        A_prev = forward_cache['A' +str(l-1)]\n",
    "\n",
    "        dA_prev, dW_curr, db_curr =linear_activation_backward(dA_curr, Z_curr, A_prev, W_curr, activation)\n",
    "\n",
    "        grads[\"dW\" +str(l)] = dW_curr\n",
    "\n",
    "        grads[\"db\" +str(l)] = db_curr\n",
    "\n",
    "        return grads\n",
    "\n",
    "def linear_activation_backward(dA, Z, A_prev, W, activation):\n",
    "\n",
    "    if activation ==\"relu\":\n",
    "\n",
    "        dZ = relu_backward(dA, Z)\n",
    "\n",
    "        dA_prev, dW, db =linear_backward(dZ, A_prev, W)\n",
    "\n",
    "    elif activation ==\"sigmoid\":\n",
    "\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "\n",
    "        dA_prev, dW, db =linear_backward(dZ, A_prev, W)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "def linear_backward(dZ, A_prev, W):\n",
    "\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "\n",
    "    db = np.sum(dZ, axis=1,keepdims=True) / m\n",
    "\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code segment 5 Backpropagation module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters)\n",
    "\n",
    "    for l in range(1, L):\n",
    "\n",
    "        parameters[\"W\" +str(l)] = parameters[\"W\" + str(l)] - learning_rate *grads[\"dW\" + str(l)]\n",
    "\n",
    "        parameters[\"b\" +str(l)] = parameters[\"b\" + str(l)] - learning_rate *grads[\"db\" + str(l)]\n",
    "\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code snippet 6 uses gradient descent to update parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, nn_architecture, learning_rate = 0.0075,num_iterations = 3000, print_cost=False):\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "# keep track of cost\n",
    "\n",
    "    costs = []\n",
    "\n",
    "# Parameters initialization.\n",
    "\n",
    "    parameters =initialize_parameters(nn_architecture)\n",
    "\n",
    "# Loop (gradient descent)\n",
    "\n",
    "    for i in range(0,num_iterations):\n",
    "\n",
    "# Forward propagation:[LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "\n",
    "        AL, forward_cache =L_model_forward(X, parameters, nn_architecture)\n",
    "\n",
    "# Compute cost.\n",
    "\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "# Backward propagation.\n",
    "\n",
    "        grads = L_model_backward(AL,Y, parameters, forward_cache, nn_architecture)\n",
    "\n",
    "# Update parameters.\n",
    "\n",
    "        parameters =update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "# Print the cost every 100training example\n",
    "\n",
    "        if print_cost and i % 100 ==0:\n",
    "\n",
    "            print(\"Cost afteriteration %i: %f\" %(i, cost))\n",
    "\n",
    "            costs.append(cost)\n",
    "\n",
    "# plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "\n",
    "    plt.ylabel('cost')\n",
    "\n",
    "    plt.xlabel('iterations (pertens)')\n",
    "\n",
    "    plt.title(\"Learning rate=\" + str(learning_rate))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code segment 7 The entire neural network model\n",
    "\n",
    "Simply applying known weights and a series of test data to the forward propagation model can predict the result.\n",
    "\n",
    "The nn_ architecture in snippet1 can be modified to build neural networks with different numbers of layers and hidden layer sizes. In addition, prepare to implement the activation function and its derived functions correctly (snippet 2). The implemented function can be used to modify the linear forward activation method in code segment 3 and the linear reverse activation method in code segment 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = L_layer_model(x_train, y_train, nn_architecture, num_iterations = 2500, print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
