{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limitations of the gradient descent method are that: the convergence rate is slow (we can see that the higher the degree of accuracy we want, the greater the number of iterations that would be required), it depends on proper initialization, It can get stuck in local optima (then you need some random re-starts), It needs a careful selection of the learning rate parameter and frequently it needs to be variable across iterations or use a line search method, it cannot be reliably applied to non convex problems, It might get close to the opimum but never converge exactly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
